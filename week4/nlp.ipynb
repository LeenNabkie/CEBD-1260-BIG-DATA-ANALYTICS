{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Category Dataset\n",
    "Identify the type of news based on headlines and short descriptions\n",
    "\n",
    "https://www.kaggle.com/rmisra/news-category-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the TECH category was selected\n",
    "df = pd.read_json('data/News_Category_Dataset_v2.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the headline and the short description\n",
    "df['headline_short_description'] = df['headline'] + ' - ' + df['short_description']\n",
    "corpus = list(df['headline_short_description'].values)\n",
    "news = corpus.copy()\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of most popular words\n",
    "from nltk.probability import FreqDist\n",
    "import itertools\n",
    "nltk.download('punkt')\n",
    "news_chart = news.copy()\n",
    "for i in range(len(news_chart)):\n",
    "    news_chart[i] = word_tokenize(news_chart[i])\n",
    "# merge a list of lists\n",
    "all_news = list(itertools.chain.from_iterable(news_chart))\n",
    "# frequency distribution\n",
    "fdist = FreqDist(all_news)\n",
    "print(fdist.most_common(30))\n",
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase, remove punctuation, spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news)):\n",
    "    # lowercase\n",
    "    news[i] = str(news[i]).lower()\n",
    "    # remove punctuation\n",
    "    translator = str.maketrans('','',string.punctuation)\n",
    "    news[i] = news[i].translate(translator)\n",
    "    # remove spaces at the begenning and at the end\n",
    "    news[i] = news[i].strip()\n",
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "for i in range(len(news)):\n",
    "    news[i] = word_tokenize(news[i])\n",
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# for a more complete list of stopwords and in other languages: https://www.ranks.nl/stopwords\n",
    "# pritn the stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "for i in range(len(news)):\n",
    "    news[i] = [word for word in news[i] if not word in stop_words]\n",
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import itertools\n",
    "\n",
    "# merge a list of lists\n",
    "all_news = list(itertools.chain.from_iterable(news))\n",
    "\n",
    "# Frequency distribution\n",
    "fdist = FreqDist(all_news)\n",
    "print(fdist.most_common(30))\n",
    "\n",
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(5):\n",
    "    stemmed_words=[]\n",
    "    for word in news[i]:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "\n",
    "    print('Original senstence:', news[i])\n",
    "    print('Stemmed sentence:', stemmed_words)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatize = WordNetLemmatizer()\n",
    "\n",
    "for i in range(5):\n",
    "    lemmatized_words=[]\n",
    "    for word in news[i]:\n",
    "        lemmatized_words.append(wordnet_lemmatize.lemmatize(word))\n",
    "\n",
    "    print('Original senstence:', news[i])\n",
    "    print('Lemmatized sentence:', lemmatized_words)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming x Lemmatization\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatize = WordNetLemmatizer()\n",
    "\n",
    "sentence = 'cities wolves children'\n",
    "tokens=nltk.word_tokenize(sentence)\n",
    "\n",
    "stemmed_words=[]\n",
    "lemmatized_words = []\n",
    "for word in tokens:\n",
    "    stemmed_words.append(stemmer.stem(word))\n",
    "    lemmatized_words.append(wordnet_lemmatize.lemmatize(word))\n",
    "\n",
    "print('Original senstence:', tokens)\n",
    "print('Stemmed sentence:', stemmed_words)\n",
    "print('Lemmatized sentence:', lemmatized_words)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Alphabetical list of part-of-speech tags\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "sentence = corpus[0]\n",
    "tokens=nltk.word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print(nltk.pos_tag(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named-entity recognition\n",
    "\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "sentence = 'Prime Minister Justin Trudeau campaigns in Montreal riding of Outremont ahead of byelection'\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonyms \n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "for ss in wordnet.synsets('small'):\n",
    "    print('Name:', ss.name())\n",
    "    print('Synonyms:', ss.lemma_names())\n",
    "    print('Definition:', ss.definition())\n",
    "    print('Examples:', ss.examples()) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df_counts = pd.DataFrame(X.toarray())\n",
    "df_counts.columns = vectorizer.get_feature_names()\n",
    "df_count = df_counts.sum(axis=0).sort_values(ascending=False).reset_index()\n",
    "df_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df_counts = pd.DataFrame(X.toarray())\n",
    "df_counts.columns = vectorizer.get_feature_names()\n",
    "df_count = df_counts.sum(axis=0).sort_values(ascending=False).reset_index()\n",
    "df_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(lowercase = True, strip_accents = 'ascii', stop_words = 'english', ngram_range = (1,2), min_df = 10)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df_counts = pd.DataFrame(X.toarray())\n",
    "df_counts.columns = vectorizer.get_feature_names()\n",
    "df_count = df_counts.sum(axis=0).sort_values(ascending=False).reset_index()\n",
    "df_count['rank'] = df_count[0].rank(axis=0, ascending=False)\n",
    "df_count.columns = ['word', 'count', 'rank_count']\n",
    "df_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase = True, strip_accents = 'ascii', stop_words = 'english', ngram_range = (1,2), min_df = 10)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df_tfidf = pd.DataFrame(X.toarray())\n",
    "df_tfidf.columns = vectorizer.get_feature_names()\n",
    "df_tfidf.sort_values('zuckerberg', ascending=False).head()\n",
    "df_tfidf = df_tfidf.sum(axis=0).sort_values(ascending=False).reset_index()\n",
    "df_tfidf['rank'] = df_tfidf[0].rank(axis=0, ascending=False)\n",
    "df_tfidf.columns = ['word', 'tfidf', 'rank_tfidf']\n",
    "df_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_count.merge(df_tfidf, on='word', how='left')\n",
    "df_compare['diff'] = abs(df_compare['rank_tfidf']-df_compare['rank_count'])\n",
    "df_compare = df_compare[(df_compare['rank_tfidf']<50)&(df_compare['rank_count']<50)]\n",
    "df_compare.sort_values('diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
